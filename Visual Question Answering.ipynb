{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSk_eSrhiSxn"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJhxvQVwqvyw"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MncoAIb5iXft"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Generating plots of images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Scikit learn \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2fAA39LibvP",
    "outputId": "2ff0c844-50e1-4eb5-edb8-b0c9a176c408"
   },
   "outputs": [],
   "source": [
    "annotation_zip = tf.keras.utils.get_file('captions.zip', \n",
    "                                         cache_subdir=os.path.abspath('.'),\n",
    "                                         origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                         extract = True)\n",
    "annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "\n",
    "name_of_zip = 'train2014.zip'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n",
    "  image_zip = tf.keras.utils.get_file(name_of_zip,\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip) + '/train2014/'\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + '/train2014/'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RutgFQniw6h",
    "outputId": "5c173a1e-f09a-47c7-acca-2d8d8a6f42e1"
   },
   "outputs": [],
   "source": [
    "print(PATH)\n",
    "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\n",
    "! unzip -a v2_Questions_Train_mscoco.zip\n",
    "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\n",
    "! unzip -a v2_Annotations_Train_mscoco.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pu9OTq8bnKIV"
   },
   "outputs": [],
   "source": [
    "#storing caption and image name in vectors\n",
    "import collections\n",
    "import operator\n",
    "\n",
    "annotation_file = 'v2_mscoco_train2014_annotations.json'\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "  annotations = json.load(f)\n",
    "\n",
    "all_answers = []\n",
    "all_answers_qids = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "  #print(annot)\n",
    "  ans_dic = collections.defaultdict(int)\n",
    "  for each in annot['answers']:\n",
    "    diffans = each['answer']\n",
    "    if diffans in ans_dic:\n",
    "      #print(each['answer_confidence'])\n",
    "      if each ['answer_confidence'] =='yes':\n",
    "        ans_dic[diffans]+=4\n",
    "      if each ['answer_confidence'] =='maybe':\n",
    "        ans_dic[diffans]+=2\n",
    "      if each ['answer_confidence'] =='no':\n",
    "        ans_dic[diffans]+=1\n",
    "    else:\n",
    "      if each ['answer_confidence'] =='yes':\n",
    "        ans_dic[diffans] = 4\n",
    "      if each ['answer_confidence'] =='maybe':\n",
    "        ans_dic[diffans] = 2\n",
    "      if each ['answer_confidence'] =='no':\n",
    "        ans_dic[diffans] = 1\n",
    "  #print(ans_dic)\n",
    "  most_fav = max(ans_dic.items(), key=operator.itemgetter(1))[0]\n",
    "  #print(most_fav)\n",
    "  caption = '<start> ' + most_fav + '<end> ' #each['answer']\n",
    "\n",
    "  image_id = annot['image_id']\n",
    "  question_id = annot['question_id']\n",
    "  full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "  all_img_name_vector.append(full_coco_image_path)\n",
    "  all_answers.append(caption)\n",
    "  all_answers_qids.append(question_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASrBbmt4nRa_"
   },
   "outputs": [],
   "source": [
    "#read json file\n",
    "question_file = 'v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "with open(question_file, 'r') as f:\n",
    "  questions=json.load(f)\n",
    "\n",
    "#storing captions and image name in vectors\n",
    "question_ids = []\n",
    "all_questions =[]\n",
    "all_img_name_vector_2 = []\n",
    "\n",
    "for annot in questions['questions']:\n",
    "  caption = '<start> ' + annot['question'] + ' <end>'\n",
    "  image_id = annot['image_id']\n",
    "  full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "  all_img_name_vector_2.append(full_coco_image_path)\n",
    "  all_questions.append(caption)\n",
    "  question_ids.append(annot['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aM7DXkBnW8l",
    "outputId": "6b1bc9de-3666-435e-8450-d6e269577bab"
   },
   "outputs": [],
   "source": [
    "print(len(all_img_name_vector), len(all_answers), len(all_answers_qids))\n",
    "print(all_img_name_vector[10:15], all_answers[10:15], all_answers_qids[10:15])\n",
    "print(len(all_img_name_vector), len(all_questions), len(question_ids))\n",
    "print(all_img_name_vector_2[10:15], all_questions[10:15], question_ids[10:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdDgFZbinaJJ",
    "outputId": "7cd84c72-5b20-432e-f39d-e439dd76c0cf"
   },
   "outputs": [],
   "source": [
    "train_answers, train_questions, img_name_vector = shuffle(all_answers, all_questions,\n",
    "                                                          all_img_name_vector,\n",
    "                                                          random_state=1)\n",
    "num_examples = 1000\n",
    "train_answers = train_answers[:num_examples]\n",
    "train_questions = train_questions[:num_examples]\n",
    "img_name_vector = img_name_vector[:num_examples]\n",
    "\n",
    "print(img_name_vector[0], train_questions[0], train_answers[0])\n",
    "\n",
    "print(len(img_name_vector), len(train_questions), len(train_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3HbByMsnhqF"
   },
   "source": [
    "Image Feature Vector using VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntbsSjfwnest"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  img = tf.image.resize(img, (299,299))\n",
    "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "  return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0KwZOEsnnON",
    "outputId": "c876f1d3-808d-4755-9bb5-57f47d067d5c"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top = False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KsThTZBnpWa"
   },
   "outputs": [],
   "source": [
    "#getting unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "#changing batch_size according to system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "# print(batch_features.shape)\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwrVkQoOnt4M"
   },
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "  return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nHg4qPIpjns",
    "outputId": "35946194-76f2-4971-fcea-920ed3cf880e"
   },
   "outputs": [],
   "source": [
    "# choosing top 10000 words from vocab\n",
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[/]^_`{|}~')\n",
    "tokenizer.fit_on_texts(train_questions)\n",
    "train_questions_seqs = tokenizer.texts_to_sequences(train_questions)\n",
    "\n",
    "#new edit\n",
    "print(tokenizer.word_index)\n",
    "ques_vocab = tokenizer.word_index\n",
    "#print(train_question_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEqLyMC9pp-d"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MRExkJypv92"
   },
   "outputs": [],
   "source": [
    "#creating tokenized vectors\n",
    "train_questions_seqs = tokenizer.texts_to_sequences(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGhnbbYvpzOr"
   },
   "outputs": [],
   "source": [
    "#padding each vector to max_len of captions\n",
    "#if max_len paramter is not provided , pad_sequences calculates that automatically\n",
    "question_vector = tf.keras.preprocessing.sequence.pad_sequences(train_questions_seqs, padding='post')\n",
    "#cap_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7GbH2JOp3BE",
    "outputId": "50ff4349-999b-4506-8409-b585df2084d6"
   },
   "outputs": [],
   "source": [
    "#calculating max_len\n",
    "#used to store attention weights\n",
    "max_length = calc_max_length(train_questions_seqs)\n",
    "print(max_length)\n",
    "\n",
    "#new edit\n",
    "max_q = max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Kv1tvwXp8fe"
   },
   "source": [
    "Creating answers one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c34N0b0Rp5qz",
    "outputId": "4eccdf09-c0d0-4307-c85c-25ef6b496703"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data=train_answers\n",
    "values=array(data)\n",
    "print(values[:10])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "ans_vocab = {l: i for i, l in enumerate(label_encoder.classes_)}\n",
    "print(ans_vocab)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse = False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print(onehot_encoded[0], len(onehot_encoded))\n",
    "\n",
    "answer_vector = onehot_encoded\n",
    "len_ans_vocab = len(onehot_encoded[0])\n",
    "\n",
    "# answer_vector = integer_encoded\n",
    "# ans_vocab = {l: i for i, l in enumerate(label_encoder.classes_)}\n",
    "print(answer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1goI_oAtqEjV",
    "outputId": "6c87b512-59eb-4f4a-a6a9-e08d4e0678b7"
   },
   "outputs": [],
   "source": [
    "print(len(question_vector[0]), len(answer_vector[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx4BvaY2qMYF"
   },
   "source": [
    "Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfw5lx_4qJ-r"
   },
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, question_train, question_val, answer_train, answer_val = train_test_split(img_name_vector,\n",
    "                                                                                                        question_vector,\n",
    "                                                                                                        answer_vector,\n",
    "                                                                                                        test_size=0.2,\n",
    "                                                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0u3aHHhqRA8",
    "outputId": "c3785b46-0270-442e-9744-fe0652250951"
   },
   "outputs": [],
   "source": [
    "print(len(img_name_train), len(img_name_val), len(question_train), len(question_val), len(answer_train), len(answer_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syiosNssqT-s",
    "outputId": "52de5df5-7636-456b-cc6d-05fe1de39eab"
   },
   "outputs": [],
   "source": [
    "question_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bNlgKkHqXEt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zX4eLSRqWIo"
   },
   "outputs": [],
   "source": [
    "# changing parameters according to system configuration\n",
    "BATCH_SIZE = 16 #64\n",
    "BUFFER_SIZE = 1 #1000\n",
    "#embedding_dim = 256\n",
    "#units = 512\n",
    "#vocab_size = len(tokenizer.word_index)+1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "#shape of vector from InceptionV3 is (64,2048)\n",
    "#these two variables represent that \n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh5ulUyJqdiC"
   },
   "outputs": [],
   "source": [
    "#loading the numpy files\n",
    "def map_func(img_name, cap, ans):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhD30MnIqgY2"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, question_train.astype(np.float32), answer_train.astype(np.float32)))\n",
    "\n",
    "#using map to load the numpy file in parallel\n",
    "dataset = dataset.map(lambda item1, item2, item3: tf.numpy_function(map_func, [item1, item2, item3], [tf.float32, tf.float32, tf.float32]),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#shuffling and batching\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJFWmIjdqipt"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, question_val.astype(np.float32), answer_val.astype(np.float32)))\n",
    "\n",
    "#using map to load numpy file in parallel\n",
    "test_dataset = test_dataset.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "    map_func, [item1, item2, item3], [tf.float32, tf.float32, tf.float32]),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWediW0xqoqf",
    "outputId": "8809cc38-e6c7-4f84-ade4-6fd7beaf50c3"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STYa19gyqtVF"
   },
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtHpQMSyqt7J"
   },
   "source": [
    "Appending Image as Word Model Treats image as word and inserts it at the end of the question. The updated word sequence is then fed to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jw18Z0nZqq69"
   },
   "outputs": [],
   "source": [
    "class AppendImageAsWordModel(tf.keras.Model):\n",
    "  def __init__(self, embedding_size, rnn_size, output_size):\n",
    "    super(AppendImageAsWordModel, self).__init__()\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.condense = tf.keras.layers.Dense(embedding_size, activation='relu')\n",
    "\n",
    "    #add embedding layers for questions\n",
    "    self.embedding = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_size)\n",
    "\n",
    "    #creating input\n",
    "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
    "                                   return_sequences = False,\n",
    "                                   return_state = False)\n",
    "    self.logits=tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "  \n",
    "  def call(self, x, sents, hidden):\n",
    "    flattened_output = self.flatten(x)\n",
    "    condensed_out = self.condense(flattened_output)\n",
    "    #print(condensed_out.shape)\n",
    "    condensed_out = tf.expand_dims(condensed_out, axis=1)\n",
    "    #print(condensed_out.shape)\n",
    "    sents = self.embedding(sents)\n",
    "    #print(sents.shape)\n",
    "    input_s = tf.concat([sents, condensed_out], axis=1)\n",
    "    #print (input_s.shape)\n",
    "    output = self.gru(input_s, initial_state = hidden)\n",
    "    final_output = self.logits(output)\n",
    "    #print(final_output.shape)\n",
    "    return final_output\n",
    "  \n",
    "  def init_state(self, batch_size, rnn_size):\n",
    "    return tf.zeros((batch_size, rnn_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcjCGmPkq5fQ",
    "outputId": "7fed98e8-1d54-4f01-c4ad-4032d0f1044f"
   },
   "outputs": [],
   "source": [
    "append_image_word_model = AppendImageAsWordModel(256, 256, len_ans_vocab)\n",
    "crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def calc_loss(labels, logits):\n",
    "  return crossentropy(labels, logits)\n",
    "\n",
    "optimizer_append = tf.keras.optimizers.Adam()\n",
    "\n",
    "# @tf.function\n",
    "def train_step(input_imgs, input_sents, labels, initial_state):\n",
    "  with tf.GradientTape() as tape:\n",
    "    my_model_output = append_image_word_model(input_imgs, input_sents, initial_state)\n",
    "    loss = calc_loss(labels, my_model_output)\n",
    "  variables = append_image_word_model.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer_append.apply_gradients(zip(gradients, variables))\n",
    "  return loss\n",
    "\n",
    "EPOCHS = 40\n",
    "for epoch in range(EPOCHS):\n",
    "  init_states = append_image_word_model.init_state(BATCH_SIZE, 256)\n",
    "  for (batch, (img_tensor, question, answer)) in enumerate(dataset):\n",
    "    loss = train_step(img_tensor, question, answer, init_states)\n",
    "  if epoch%10==0:\n",
    "    print(\"Epoch #%d, Loss %.4f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHLsxNLUrdbE"
   },
   "source": [
    "Model 2 \n",
    "Separate Image as Word Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcR_w3zCq-m1"
   },
   "outputs": [],
   "source": [
    "class SeparateImageAsWordModel(tf.keras.Model):\n",
    "  def __init__(self, embedding_size, rnn_size, output_size):\n",
    "    super(SeparateImageAsWordModel, self).__init__()\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.condense = tf.keras.layers.Dense(embedding_size, activation='relu')\n",
    "\n",
    "    #add embedding layer for questions\n",
    "    self.embedding = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_size)\n",
    "\n",
    "    #create the input\n",
    "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
    "                                   return_sequences = False,\n",
    "                                   return_state = False)\n",
    "    self.logits = tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "\n",
    "  def call(self, x, snets, hidden):\n",
    "    flattened_output = self.flatten(x)\n",
    "    condensed_out = self.condense(flattened_output)\n",
    "\n",
    "    #print(condensed_out.shape)\n",
    "    condensed_out = tf.expand_dims(condensed_out, axis = 1)\n",
    "    #print(condensed_out.shape)\n",
    "    sents = self.embedding(sents)\n",
    "    sent_lstm_output = self.gru(sents, initial_state=hidden) #run lstm on questions sent\n",
    "    sent_lstm_output = tf.expand_dims(sent_lstm_output, axis=1)\n",
    "    #print(sent_lstm_output.shape)\n",
    "    output = tf.concat([sent_lstm_output, condensed_out], axis=2) #word and image embedding side by side\n",
    "    #print(output.shape)\n",
    "    final_output = self.logits(output)\n",
    "    return final_output\n",
    "\n",
    "  def init_state(self, batch_size, rnn_size):\n",
    "    return tf.zeros((batch_size, rnn_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KIF6ODLyaac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_ugsca-0Uwd"
   },
   "source": [
    "Model 3 Co Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QeXrB1izMZ5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Embedding, LSTM, Activation, ZeroPadding1D, Conv1D\n",
    "\n",
    "class CoattentionModel(tf.keras.Model):\n",
    "  def __init__(self, ans_vocab, max_q, ques_vocab):\n",
    "    super(CoattentionModel, self).__init__(name='CoattentionModel')\n",
    "    self.ans_vocab = ans_vocab\n",
    "    self.max_q = max_q\n",
    "    self.ques_vocab = ques_vocab\n",
    "\n",
    "    self.ip_dense = Dense(256, activation ='relu', input_shape=(512, ))\n",
    "    num_words = len(ques_vocab)+2\n",
    "    self.word_level_feats = Embedding(input_dim = len(ques_vocab)+2, output_dim=256)\n",
    "    self.lstm_layer = LSTM(256, return_sequences=True, input_shape = (None, max_q, 256))\n",
    "    self.dropout_layer = Dropout(0.5)\n",
    "    self.tan_layer = Activation('tanh')\n",
    "    self.dense_image = Dense(256, activation = 'relu', input_shape = (256,))\n",
    "    self.dense_text = Dense(256, activation = 'relu', input_shape = (256,))\n",
    "    self.image_attention = Dense(1, activation = 'softmax', input_shape = (256,))\n",
    "    self.text_attention = Dense(1, activation = 'softmax', input_shape = (256,))\n",
    "    self.dense_word_level = Dense(256, activation = 'relu', input_shape = (256,))\n",
    "    self.dense_phrase_level = Dense(256, activation = 'relu', input_shape = (2*256,))\n",
    "    self.dense_sent_level = Dense(256, activation = 'relu', input_shape = (2*256,))\n",
    "    self.dense_final = Dense(len(ans_vocab), activation = 'relu', input_shape = (256,))\n",
    "\n",
    "  def affinity(self, image_feat, text_feat, level, prev_att):\n",
    "    img = self.dense_image(image_feat)\n",
    "    text = self.dense_text(text_feat)\n",
    "\n",
    "    if level==0:\n",
    "      return self.dropout_layer(self.tan_layer(text))\n",
    "\n",
    "    elif level==1:\n",
    "      level=tf.expand_dims(self.dense_text(prev_att), 1)\n",
    "      return self.dropout_layer(self.tan_layer(img+level))\n",
    "\n",
    "    elif level==2:\n",
    "      level=tf.expand_dims(self.dense_image(prev_att), 1)\n",
    "      return self.dropout_layer(self.tan_layer(text+level))\n",
    "        \n",
    "  def attention_ques(self, text_feat, text):\n",
    "    return tf.reduce_sum(self.text_attention(text) * text_feat,1)\n",
    "\n",
    "  def attention_img(self, image_feat, img):\n",
    "    return tf.reduce_sum(self.image_attention(img) * image_feat,1)\n",
    "    \n",
    "  def call(self, image_feat, question_encoding):\n",
    "    #Preprocessing the image\n",
    "    image_feat = self.ip_dense(image_feat)\n",
    "\n",
    "    #Text Features\n",
    "\n",
    "    #Text: Word Level\n",
    "    word_feat = self.word_level_feats(question_encoding)\n",
    "\n",
    "    #Text: Sentence Level\n",
    "    sent_feat = self.lstm_layer(word_feat)\n",
    "\n",
    "    #Apply attention on word level features\n",
    "    word_text_attention = self.attention_ques(word_feat, self.affinity(image_feat, word_feat,0,0))\n",
    "    word_img_attention = self.attention_img(image_feat, self.affinity(image_feat, word_feat,1,word_text_attention))\n",
    "    word_text_attention = self.attention_ques(word_feat, self.affinity(image_feat, word_feat,2,word_img_attention))\n",
    "\n",
    "    word_pred = self.dropout_layer(self.tan_layer(self.dense_word_level(word_img_attention + word_text_attention)))\n",
    "\n",
    "    #Applying attention on sentence level features\n",
    "    sent_text_attention = self.attention_ques(sent_feat, self.affinity(image_feat, sent_feat,0,0))\n",
    "    sent_img_attention = self.attention_img(image_feat, self.affinity(image_feat, sent_feat,1,sent_text_attention))\n",
    "    sent_text_attention = self.attention_ques(sent_feat, self.affinity(image_feat, sent_feat,2,sent_img_attention))\n",
    "\n",
    "    sent_pred = self.dropout_layer(self.tan_layer(self.dense_sent_level(tf.concat([sent_img_attention + sent_text_attention, word_pred], -1))))\n",
    "\n",
    "    return self.dense_final(sent_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppLjscZ6AmgY"
   },
   "outputs": [],
   "source": [
    "model = CoattentionModel(ans_vocab, max_q, ques_vocab)\n",
    "\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True) #sparse\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss_metric = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "test_accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSAnj1PzCP3K"
   },
   "outputs": [],
   "source": [
    "def train_step(images, questions, answers, model):\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    #Forward pass\n",
    "    predictions = model(images, questions)\n",
    "    train_loss = loss_function(y_true=answers, y_pred=predictions)\n",
    "\n",
    "    #Backward Pass\n",
    "    gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    #Record Results\n",
    "    train_loss_metric(train_loss)\n",
    "    train_accuracy_metric(answers, predictions)\n",
    "\n",
    "    def test_step(images, questions, answers, model):\n",
    "      predictions = model(images, questions)\n",
    "      test_loss = loss_function(y_true=answers, y_pred=predictions)\n",
    "\n",
    "      #Record Results\n",
    "      test_loss_metric(test_loss)\n",
    "      test_accuracy_metric(answers, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "Ez5cAtbwJ8nw",
    "outputId": "7eb55bd2-af11-48e9-e1c4-79936481024e"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for epoch in range(EPOCHS):\n",
    "  for (batch, (img_tensor, question, answer)) in enumerate(dataset):\n",
    "  #for images, labels in mnist train:\n",
    "    train_step(img_tensor, question, answer, model)\n",
    "  #for test_images, test_labels in mnist dataset:\n",
    "  for (batch, (img_tensor, question, answer)) in enumerate(test_dataset):\n",
    "    test_step(img_tensor, question, answer,model)\n",
    "\n",
    "  template = 'Epoch : {}, Loss : {:.4f}, Accuracy : {:.2f}, Test Loss: {:.4f}, Test Accuracy : {:.2f}'\n",
    "  train_loss.append(train_loss_metric.result())\n",
    "  test_loss.append(test_loss_metric.result())\n",
    "  train_acc.append(train_accuracy_metric.result()*100)\n",
    "  test_acc.append(test_accuracy_metric.result()*100)\n",
    "  print(template.format(epoch+1, \n",
    "                        train_loss_metric.result(),\n",
    "                        train_accuracy_metric.result()*100,\n",
    "                        test_loss_metric.result(),\n",
    "                        test_accuracy_metric.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_YlQB2lNC3C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MainLSTMCode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
